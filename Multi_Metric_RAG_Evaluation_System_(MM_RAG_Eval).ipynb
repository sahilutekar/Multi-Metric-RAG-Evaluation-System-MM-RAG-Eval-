{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Metric RAG Evaluation System (MM-RAG-Eval)\n",
        "\n",
        "Focuses on the fact that you are measuring everything together:\n",
        "\n",
        "Accuracy vs GT\n",
        "\n",
        "Hallucination\n",
        "\n",
        "Robustness\n",
        "\n",
        "Similarity metrics\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D87umKFaiyXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXb35B7TZpiB"
      },
      "outputs": [],
      "source": [
        "# Optimized RAG Hyperparameter Evaluation Pipeline (memory-friendly)\n",
        "# ----------------------------------------------------------------\n",
        "# - Loads LLM once (with max context window)\n",
        "# - Builds VectorStoreIndex once per chunk size (caches indices)\n",
        "# - Evaluates each (temp, chunk, context) config reusing the index & model\n",
        "# - Uses prompt-based 'temperature emulation' to avoid reloading LLM for each temp\n",
        "#\n",
        "# Notes:\n",
        "# - If you want *strict numeric* temperature changes, uncomment the LLM re-init block\n",
        "#   (but that reloads the heavy model per temperature and will increase RAM/time).\n",
        "# - For production evaluation you may prefer to load multiple LLMs on GPU nodes or use\n",
        "#   an API (OpenAI, Vertex) to vary temperature cheaply.\n",
        "#\n",
        "# Requirements (install in Colab/Jupyter before running):\n",
        "# !pip install -q llama-index ragas datasets rouge-score nltk sentencepiece transformers\n",
        "# !pip install -q llama-cpp-python  # optional: only if using LlamaCPP locally\n",
        "# nltk data:\n",
        "# import nltk; nltk.download(\"punkt\")\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# NLP / metrics\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# LlamaIndex imports\n",
        "from llama_index.core import Settings, VectorStoreIndex, Document, SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"optimized_rag_eval\")\n",
        "\n",
        "# -------------------------\n",
        "# 1. User Data & Ground Truth\n",
        "# -------------------------\n",
        "DOCUMENT_SOURCES = [\n",
        "    \"/content/book (1).pdf\",        # local PDF (ensure exists)\n",
        "    {\"text\": \"\"\"Cancer is a disease in which abnormal cells divide uncontrollably and can invade nearby tissues.\n",
        "These abnormal cells may also spread to other parts of the body through the blood and lymph systems.\n",
        "Cancer can start almost anywhere in the human body. There are many types of cancer including breast cancer,\n",
        "lung cancer, prostate cancer, and blood cancers like leukemia.\n",
        "\n",
        "Common symptoms of cancer include unexplained weight loss, fatigue, lumps, prolonged cough, and changes in bowel habits.\n",
        "Treatment options include chemotherapy, radiation, surgery, immunotherapy, and targeted therapy.\n",
        "Early detection significantly improves survival rates.\n",
        "\"\"\"},\n",
        "]\n",
        "\n",
        "TEST_QUESTIONS = [\n",
        "    \"What is cancer?\",\n",
        "    \"What are common symptoms of cancer?\",\n",
        "    \"How can cancer spread in the body?\"\n",
        "]\n",
        "\n",
        "GROUND_TRUTH = {\n",
        "    \"What is cancer?\":\n",
        "        \"Cancer is a disease where abnormal cells divide uncontrollably and invade nearby tissues.\",\n",
        "    \"What are common symptoms of cancer?\":\n",
        "        \"Common symptoms include weight loss, fatigue, lumps, cough, and changes in bowel habits.\",\n",
        "    \"How can cancer spread in the body?\":\n",
        "        \"Cancer can spread through the blood and lymphatic systems to other parts of the body.\",\n",
        "}\n",
        "\n",
        "# -------------------------\n",
        "# 2. Hyperparameter grid (edit)\n",
        "# -------------------------\n",
        "# temperatures = [0.1, 0.7, 1.0]          # we will emulate these via prompt instructions (faster)\n",
        "# chunk_sizes = [500, 300, 200]          # build one index per chunk size (cached)\n",
        "# # choose the max context window you want to support and load the LLM with that\n",
        "context_windows = [8000, 4000, 2000]   # for the model we will use the max 8000 (loaded once)\n",
        "\n",
        "temperatures = [0.1, 0.5, 0.9]\n",
        "max_new_tokens_list = [128, 256, 512]\n",
        "chunk_sizes = [128, 256, 512]\n",
        "# -------------------------\n",
        "# 3. Embedding & Metric utils (pre-initialize to avoid repeated overhead)\n",
        "# -------------------------\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\")\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    try:\n",
        "        v1 = embed_model.get_text_embedding(a)\n",
        "        v2 = embed_model.get_text_embedding(b)\n",
        "        return float(cosine_similarity([v1], [v2])[0][0])\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Cosine embedding error: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def compute_rouge(pred, ref):\n",
        "    try:\n",
        "        sc = rouge.score(ref, pred)\n",
        "        return float(sc[\"rouge1\"].fmeasure), float(sc[\"rougeL\"].fmeasure)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"ROUGE error: {e}\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "def compute_bleu(pred, ref):\n",
        "    try:\n",
        "        pt = word_tokenize(pred.lower())\n",
        "        rt = [word_tokenize(ref.lower())]\n",
        "        return float(sentence_bleu(rt, pt, smoothing_function=smooth))\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# Heuristic classification of accuracy\n",
        "def classify_accuracy(cosine_score, rouge1, bleu):\n",
        "    if cosine_score >= 0.75 or (rouge1 >= 0.5 and bleu >= 0.4):\n",
        "        return \"Yes\"\n",
        "    if cosine_score >= 0.4 or rouge1 >= 0.25 or bleu >= 0.15:\n",
        "        return \"Partial\"\n",
        "    return \"No\"\n",
        "\n",
        "def hallucination_from_accuracy(acc_label):\n",
        "    return {\"Yes\": \"Low\", \"Partial\": \"Medium\", \"No\": \"High\"}[acc_label]\n",
        "\n",
        "def robustness_score(acc_label, hall_label):\n",
        "    acc_map = {\"Yes\": 1.0, \"Partial\": 0.6, \"No\": 0.0}\n",
        "    hall_map = {\"Low\": 1.0, \"Medium\": 0.5, \"High\": 0.0}\n",
        "    return round((0.7 * acc_map[acc_label]) + (0.3 * hall_map[hall_label]), 2)\n",
        "\n",
        "# -------------------------\n",
        "# 4. Build documents once (load PDF + text)\n",
        "# -------------------------\n",
        "def build_documents(sources):\n",
        "    docs = []\n",
        "    for s in sources:\n",
        "        if isinstance(s, str) and os.path.exists(s):\n",
        "            try:\n",
        "                reader = SimpleDirectoryReader(input_files=[s])\n",
        "                loaded = reader.load_data()\n",
        "                docs.extend(loaded)\n",
        "                logger.info(f\"Loaded {len(loaded)} docs from {s}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to load {s}: {e}\")\n",
        "        elif isinstance(s, dict) and \"text\" in s:\n",
        "            docs.append(Document(text=s[\"text\"]))\n",
        "        else:\n",
        "            logger.info(f\"Skipping source: {s}\")\n",
        "    return docs\n",
        "\n",
        "docs = build_documents(DOCUMENT_SOURCES)\n",
        "if not docs:\n",
        "    raise RuntimeError(\"No documents found. Check DOCUMENT_SOURCES.\")\n",
        "\n",
        "# -------------------------\n",
        "# 5. Build & cache indices for each chunk size (only once)\n",
        "# -------------------------\n",
        "index_cache = {}\n",
        "for csize in set(chunk_sizes):\n",
        "    Settings.chunk_size = csize\n",
        "    logger.info(f\"Building index for chunk_size={csize} (this may take a moment)\")\n",
        "    idx = VectorStoreIndex.from_documents(docs)\n",
        "    index_cache[csize] = idx\n",
        "logger.info(\"Index cache built for chunk sizes: \" + \", \".join(map(str, index_cache.keys())))\n",
        "\n",
        "# -------------------------\n",
        "# 6. Load the LLM once with the MAX context window (memory efficient)\n",
        "#     - we set context_window to max(context_windows) to support all runs\n",
        "# -------------------------\n",
        "max_context = max(context_windows)\n",
        "# Replace model_url below with your local GGUF path if you have one (recommended for speed)\n",
        "LLM_MODEL_URL = None  # e.g. \"/mnt/models/mistral-7b-instruct.gguf\" or keep None to use default remote (slower)\n",
        "\n",
        "llm_kwargs = dict(\n",
        "    model_url = LLM_MODEL_URL or 'https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
        "    temperature = 0.1,                 # default; we will emulate other temps via prompt (explained)\n",
        "    max_new_tokens = 256,\n",
        "    context_window = max_context,\n",
        "    messages_to_prompt = messages_to_prompt,\n",
        "    completion_to_prompt = completion_to_prompt,\n",
        "    verbose = False,\n",
        ")\n",
        "\n",
        "logger.info(\"Loading LLM once (this is the heaviest step).\")\n",
        "try:\n",
        "    llm = LlamaCPP(**llm_kwargs)\n",
        "    Settings.llm = llm\n",
        "    logger.info(\"LLM loaded.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Failed to initialize LlamaCPP. If you don't have llama-cpp installed or model accessible, switch to an API-based LLM. Error: \" + str(e))\n",
        "\n",
        "# -------------------------\n",
        "# 7. Query helper: uses index (cached) and the single LLM\n",
        "#    We use a small prompt-prefix to hint at temperature style instead of reloading LLM for each temperature.\n",
        "#    If you want exact numeric temperature behavior, see the commented 'strict-temp' option below.\n",
        "# -------------------------\n",
        "def temperature_prompt_hint(temp):\n",
        "    # conservative -> low-temp style; balanced -> mid; creative -> high-temp style\n",
        "    if temp <= 0.2:\n",
        "        return \"Answer concisely and factually. Do not add extra speculation or make up facts.\"\n",
        "    if temp <= 0.6:\n",
        "        return \"Answer accurately. You may paraphrase and summarize but stay factual.\"\n",
        "    return \"Answer creatively; if unsure, offer plausible hypotheses (may be speculative).\"\n",
        "\n",
        "def query_with_index(index, question, temp_hint):\n",
        "    \"\"\"\n",
        "    Query the provided index with a prompt-level temperature hint.\n",
        "    Returns the model's raw string.\n",
        "    \"\"\"\n",
        "    preface = temp_hint + \"\\n\\nQuestion: \" + question + \"\\nAnswer:\"\n",
        "    qe = index.as_query_engine()\n",
        "    resp = qe.query(preface)\n",
        "    return str(resp)\n",
        "\n",
        "# ---------- Optional: strict numeric temp (uncomment to enable)\n",
        "# WARNING: enabling this will reinitialize LlamaCPP per temperature (heavy)\n",
        "# def real_query_strict_temp(index, question, temp, context_window):\n",
        "#     Settings.llm = None\n",
        "#     llm_local = LlamaCPP(model_url=LLM_MODEL_URL or '...', temperature=temp, context_window=context_window, max_new_tokens=256, messages_to_prompt=messages_to_prompt, completion_to_prompt=completion_to_prompt)\n",
        "#     Settings.llm = llm_local\n",
        "#     qe = index.as_query_engine()\n",
        "#     resp = qe.query(question)\n",
        "#     return str(resp)\n",
        "# ----------\n",
        "\n",
        "# -------------------------\n",
        "# 8. Main sweep: iterate combos, evaluate per-question, aggregate\n",
        "# -------------------------\n",
        "results = []\n",
        "per_question_logs = []\n",
        "\n",
        "start_all = time.time()\n",
        "for temp in temperatures:\n",
        "    temp_hint = temperature_prompt_hint(temp)   # emulate temperature via instruction\n",
        "    for csize in chunk_sizes:\n",
        "        index = index_cache[csize]  # reuse cached index for this chunk size\n",
        "        # We loaded the LLM with max_context. If you need to restrict context_window in responses,\n",
        "        # either reinit the model (heavy) or accept that model sees the larger context (usually fine).\n",
        "        context_window_used = max_context\n",
        "\n",
        "        # Evaluate all questions for this (temp, chunk, context)\n",
        "        acc_labels = []\n",
        "        hall_labels = []\n",
        "        robust_scores = []\n",
        "        outputs_summary = []\n",
        "\n",
        "        for q in TEST_QUESTIONS:\n",
        "            try:\n",
        "                out = query_with_index(index, q, temp_hint)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Query failed for q='{q}' (t={temp}, c={csize}): {e}\")\n",
        "                out = \"\"\n",
        "\n",
        "            # compute metrics vs GT\n",
        "            gt = GROUND_TRUTH.get(q, \"\")\n",
        "            cos = cosine_sim(out, gt) if out else 0.0\n",
        "            r1, rL = compute_rouge(out, gt) if out else (0.0, 0.0)\n",
        "            bleu = compute_bleu(out, gt) if out else 0.0\n",
        "\n",
        "            # classify\n",
        "            acc_label = classify_accuracy(cosine_score=cos, rouge1=r1, bleu=bleu)\n",
        "            hall_label = hallucination_from_accuracy(acc_label)\n",
        "            robust = robustness_score(acc_label, hall_label)\n",
        "\n",
        "            # gather\n",
        "            acc_labels.append(acc_label)\n",
        "            hall_labels.append(hall_label)\n",
        "            robust_scores.append(robust)\n",
        "            outputs_summary.append(out[:300].replace(\"\\n\", \" \"))  # short preview\n",
        "\n",
        "            per_question_logs.append({\n",
        "                \"temp\": temp, \"chunk\": csize, \"context_window\": context_window_used,\n",
        "                \"question\": q, \"output\": out, \"cosine\": cos, \"rouge1\": r1, \"rougeL\": rL, \"bleu\": bleu,\n",
        "                \"accuracy_label\": acc_label, \"hallucination_label\": hall_label, \"robustness\": robust\n",
        "            })\n",
        "\n",
        "        # aggregate across questions (majority) and avg robustness\n",
        "        def majority(lbls):\n",
        "            unique, counts = np.unique(lbls, return_counts=True)\n",
        "            return unique[np.argmax(counts)]\n",
        "\n",
        "        agg_acc = majority(acc_labels)\n",
        "        agg_hall = majority(hall_labels)\n",
        "        agg_rob = round(float(np.mean(robust_scores)), 2)\n",
        "\n",
        "        output_overall = \"Correct\" if agg_acc == \"Yes\" else (\"Partial\" if agg_acc == \"Partial\" else \"Wrong\")\n",
        "\n",
        "        results.append({\n",
        "            \"Temp\": temp,\n",
        "            \"Chunk\": csize,\n",
        "            \"Context\": context_window_used,\n",
        "            \"Output\": output_overall,\n",
        "            \"Accuracy vs GT\": agg_acc,\n",
        "            \"Hallucination\": agg_hall,\n",
        "            \"Robustness\": agg_rob,\n",
        "            \"sample_outputs\": outputs_summary\n",
        "        })\n",
        "\n",
        "end_all = time.time()\n",
        "logger.info(f\"Full sweep finished in {round(end_all - start_all, 1)}s\")\n",
        "\n",
        "# -------------------------\n",
        "# 9. Save / show results\n",
        "# -------------------------\n",
        "df = pd.DataFrame(results)\n",
        "pd.DataFrame(per_question_logs).to_csv(\"per_question_details.csv\", index=False)\n",
        "df.to_csv(\"hyperparam_matrix_optimized.csv\", index=False)\n",
        "\n",
        "print(\"\\nFinal Hyperparameter Matrix (optimized):\")\n",
        "display(df)\n",
        "\n",
        "# Optional: visual heatmap (Temp x Chunk -> Robustness)\n",
        "try:\n",
        "    pivot = df.pivot(index=\"Temp\", columns=\"Chunk\", values=\"Robustness\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.title(\"Robustness Heatmap (Temp vs Chunk)\")\n",
        "    plt.imshow(pivot.values, aspect=\"auto\")\n",
        "    plt.colorbar(label=\"Robustness\")\n",
        "    plt.xticks(range(len(pivot.columns)), pivot.columns)\n",
        "    plt.yticks(range(len(pivot.index)), pivot.index)\n",
        "    plt.xlabel(\"Chunk Size\")\n",
        "    plt.ylabel(\"Temperature\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Could not plot heatmap: {e}\")\n",
        "\n",
        "# -------------------------\n",
        "# 10. Quick tips to further reduce memory/time if needed:\n",
        "# -------------------------\n",
        "#  - Use a smaller embedding model (e.g., all-MiniLM-L6-v2) for large PDFs.\n",
        "#  - Use a smaller LLM for evaluation (distilled / 1-2B) if you only need ranking.\n",
        "#  - Cache embedding vectors to disk (so reboots don't recompute).\n",
        "#  - Run model on a GPU-enabled instance if available.\n",
        "# -------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index\n",
        "%pip install llama-index-llms-llama-cpp\n",
        "# !pip install -U langchain-community\n",
        "%pip install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "yvYUL8_PZ0PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rouge-score nltk\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# !pip install RAGAS\n",
        "\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "8ZbJAVlqZ5q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QG6cbc5ReO0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}